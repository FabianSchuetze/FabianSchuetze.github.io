<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title> - articles</title>
        <link rel="stylesheet" href="https://fabianschuetze.github.io/theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://fabianschuetze.github.io/"></a></h1>
                <nav><ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/blog/">Blog</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="https://fabianschuetze.github.io/gemmcuda.html">"GemmCuda"</a></h1>
<footer class="post-info">
        <abbr class="published" title="2024-03-14T00:00:00+01:00">
                Published: Do 14 März 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://fabianschuetze.github.io/author/fabian-schuetze.html">Fabian Schuetze</a>
        </address>
<p>In <a href="https://fabianschuetze.github.io/category/articles.html">articles</a>.</p>

</footer><!-- /.post-info --><h1>Reverse-Engeneering CuBLAS:  Achieving CuBLAS performance with Tensor Cores by Mimicking SASS Instructions</h1>
<p>This article reverse-engineers a Cuda GEMM kernel with Tensor cores by looking at the SASS code for a CuBLAS invocation. The handwritten kernel gets within striking distance of the performance of the CuBLAS kernel. Re-engineering the CuBLAS kernel is not too difficult when using good abstractions as building blocks.  </p>
<p>CuBLAS is the BLAS library provided by Nvidia to multiply matrices. The API is well documented, but the source code is not published. The kernels provided with CuBLAS are heavily tuned, and the best-performing kernel gets selected at runtime. The runtime choses among many kernels: One can count ~5000 kernels containing GEMM in its name, and CuBLAS ships a whopping 100 MB. In comparison, the BLAS library provided by Ubuntu, libblas, ships 600kb.</p>
<p>The performance of three different handwritten CUDA kernels and the CuBLAS version is shown below:</p>
<p><img alt="Performance" src="https://fabianschuetze.github.io/images/TFLOPS.png"></p>
<p>The three versions differ in their use of PTX primitives and the degree of instruction-level parallelism attained (ILP). A high ILP can be achieved by writing efficient abstractions and placing them well in the code to permit prefetching and avoiding pipeline stalls. Modern PTX instructions need to be used to permit asynchronous and highly efficient loading of global memory. This efficiency is documented by the kernels ILP, which is:</p>
<p><img alt="ILP" src="https://fabianschuetze.github.io/images/ILP.png"></p>
<p>Note, for users used to CPU optimization, the ILP is extremly high, which is explained by the extensive parallelism GPUs offer.</p>
<p>This article proceeds in the following stages:
First, the basic GEMM implementation using Tensor cores is shown.
Second, the SASS (Cuda assembly) code for the highly optimized Cuda kernel is analyzed, and differences between the instructions of the basic implementation are identified.
The basic implementation is refined in two steps to reach performance parity with Cublas.</p>
<h2>1.  Basic GEMM Implementation</h2>
<p>The main loop of the basic implementation of a GEMM kernel with tensor cores is:</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="p">;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load_blocking</span><span class="p">();</span>
<span class="w">     </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load_blocking</span><span class="p">();</span>
<span class="w">     </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">     </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">     </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">     </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">wmma_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span>
<span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">wmma_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">wmma_step</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">wmma_steps</span><span class="p">;</span><span class="w"> </span><span class="n">wmma_step</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">             </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">             </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">             </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">Bs</span><span class="p">.</span><span class="n">cols_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kN</span><span class="p">);</span>
<span class="w">             </span><span class="n">matmul</span><span class="p">.</span><span class="n">compute</span><span class="p">();</span>
<span class="w">     </span><span class="p">}</span>
<span class="w">     </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">     </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">     </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div>

<p>The code above documents the basic structure of a decent GEMM kernel with tensor cores: Looping along the K (inner) dimension of the matrix product in blocks, the kernel loads blocks of the matrices A and B into shared memory. The load function is named <code>load_blocking</code> (which already provides a glimpse at future optimizations). The kernel then uses a loop nest to compute the matrix product over these blocks. Smaller blocks of the shared memory get loaded into local register files, and its matrix product gets calculated. The kernel reaches about 60TFlops on an A5000, or ⅔ of the GPU limit.</p>
<p>The code above gets compiled to the following SASS assembly:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span>...
<span class="w">    </span>LDG.E.128.CONSTANT<span class="w"> </span>R72,<span class="w"> </span><span class="o">[</span>R72.64<span class="o">]</span>
<span class="w">    </span>...
<span class="w">    </span>WARPSYNC<span class="w"> </span>0xffffffff
<span class="w">    </span>...
<span class="w">    </span>STS.128<span class="w"> </span><span class="o">[</span>R143<span class="o">]</span>,<span class="w"> </span>R52
<span class="w">    </span>...
<span class="w">    </span>BAR.SYNC<span class="w"> </span>0x0
<span class="w">    </span>LDSM.16.M88.4<span class="w"> </span>R80,<span class="w"> </span><span class="o">[</span>R80<span class="o">]</span>
<span class="w">    </span>...
<span class="w">    </span>HMMA.16816.F16<span class="w"> </span>R18,<span class="w"> </span>R80,<span class="w"> </span>R68,<span class="w"> </span>R18
<span class="w">    </span>...
<span class="w">    </span>BAR.SYNC<span class="w"> </span>0x0
</code></pre></div>

<p>The assembly reveals the inner workings of the code above: First, <code>load_blocking</code>stores 128 bits from global memory into thread-local registers. After the global loads, all threads in the warp wait at a barrier. Then, the threads store the loaded data in shared memory, and all threads in a block sync. Furthermore, data from shared memory is loaded as a matrix for processing by the tensor cores. Then, a tensor core matrix multiplication with half-floats ensues. Finally, all threads in the block wait at a barrier before the loop starts again. The way data is loaded is pictured in the following graph:</p>
<p><img alt="Blocking" src="https://fabianschuetze.github.io/images/blocking.png"></p>
<p>From the very right, 255MB are loaded from device memory to the L2 Cache before landing in the L1 Cache. As can be seen in the top left of the figure, there are 3.41m instructions used to load data into the local registers. From the local registers, the data is stored again in the shared memory (a portion of the L1 cache) in 3.15m requests. From the shared memory, the data gets accessed in 11.53m requests.</p>
<h2>2. SASS code for CuBLAS assembly code</h2>
<p>The SASS code for the CuBLAS kernel is interesting. An abbreviated version reads as follows:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span>HMMA.16816.F32<span class="w"> </span>R0,<span class="w"> </span>R152,<span class="w"> </span>R184,<span class="w"> </span>R0
<span class="w">    </span>LDSM.16.MT88.4<span class="w"> </span>R168,<span class="w"> </span><span class="o">[</span>R137+UR8+0x800<span class="o">]</span>
<span class="w">    </span>LDGSTS.E.BYPASS.LTC128B.128.CONSTANT<span class="w"> </span><span class="o">[</span>R129+UR4+0x3000<span class="o">]</span>,<span class="w"> </span><span class="o">[</span>R130.64+0x180<span class="o">]</span>
<span class="w">    </span>...
<span class="w">    </span>HMMA.16816.F32<span class="w"> </span>R4,<span class="w"> </span>R152,<span class="w"> </span>R186,<span class="w"> </span>R4
<span class="w">    </span>HMMA.16816.F32<span class="w"> </span>R8,<span class="w"> </span>R152,<span class="w"> </span>R188,<span class="w"> </span>R8
<span class="w">    </span>...
<span class="w">    </span>HMMA.16816.F32<span class="w"> </span>R120,<span class="w"> </span>R164,<span class="w"> </span>R196,<span class="w"> </span>R120
<span class="w">    </span>DEPBAR.LE<span class="w"> </span>SB0,<span class="w"> </span>0x1
<span class="w">    </span>...
</code></pre></div>

<p>The assembly code highlights several aspects:
The main loop starts with a matrix multiplication instead of a memory load.
The global load <code>LDGSTS.E.BYPASS.LTC128B.128.CONSTANT</code> differs from the load in the basic GEMM implementation, <code>LDG.E.128.CONSTANT R72</code>: Firstly, It bypasses the register and stores the data directly in shared memory. Furthermore, it is an asynchronous load and does not block the treads. Non-blocking requires a separate memory fence to signal that the data is ready.
Such a barrier is the dependency barrier <code>DEPBAR.LE.</code>
Finally, the instructions are mixed: There is no linear separation between loading data and operating on it, but a heavy mixture of instructions.
The CuBLAS kernel achieves ~90TFLops. The following two kernels describe how to write code that produces similar SASS and attains the same performance.</p>
<h2>3. Improvement I: Buffering</h2>
<h3>Asynchronous Load Instructions</h3>
<p>Starting with <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#changes-in-ptx-isa-version-7-0">PTX Version 7.0</a> Cuda provides instructions to copy data asynchronously from global to shared memory. The copy bypasses local registers and stores data directly to the shared memory (L1 cache). As identified above, asynchronous loading is one of the differences between the simple GEMM code and the Cublas version.<br>
Two changes are necessary for asynchronous loading. First, the new load function is:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">load</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">counter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">global_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">offset_</span><span class="p">.</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ld_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset_</span><span class="p">.</span><span class="n">col</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">rows</span><span class="p">;</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">stride_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global_ptr_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ld_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">global_idx</span><span class="p">;</span>
<span class="w">                </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">shmem_</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">rows</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset_</span><span class="p">.</span><span class="n">row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">row</span><span class="p">,</span>
<span class="w">                            </span><span class="n">offset_</span><span class="p">.</span><span class="n">col</span><span class="p">);</span><span class="w"> </span><span class="c1">// + row * cols;</span>
<span class="w">                </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">load_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">                </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">pos_in_ss</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">__cvta_generic_to_shared</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">int4</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">));</span>
<span class="w">                </span><span class="n">CP_ASYNC_CG</span><span class="p">(</span><span class="n">pos_in_ss</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">load_bytes</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>The first load function was:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">load_blocking</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">global_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">offset_</span><span class="p">.</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ld_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset_</span><span class="p">.</span><span class="n">col</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">rows</span><span class="p">;</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">stride_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global_ptr_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ld_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">global_idx</span><span class="p">;</span>
<span class="w">                </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">shmem_</span><span class="p">(</span><span class="n">offset_</span><span class="p">.</span><span class="n">row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">row</span><span class="p">,</span>
<span class="w">                            </span><span class="n">offset_</span><span class="p">.</span><span class="n">col</span><span class="p">);</span><span class="w"> </span><span class="c1">// + row * cols;</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="n">int4</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="n">int4</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">src</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">                </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">int4</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>The <code>load_blocking</code> function loads 128bit by casting eight half floats as an int4 and loads it. In contrast, the <code>load</code> function uses the macro <code>CP_ASTNC_CG</code> comprising the following PTX instructions:</p>
<div class="highlight"><pre><span></span><code><span class="c1">#define CP_ASYNC_CG(dst, src, Bytes)    \</span>
<span class="w">    </span>asm<span class="w"> </span>volatile<span class="o">(</span><span class="w">   </span><span class="se">\</span>
<span class="w">        </span><span class="s2">&quot;cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\n&quot;</span><span class="w"> </span>::<span class="s2">&quot;r&quot;</span><span class="o">(</span>dst<span class="o">)</span>,<span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="s2">&quot;l&quot;</span><span class="o">(</span>src<span class="o">)</span>,<span class="w"> </span><span class="s2">&quot;n&quot;</span><span class="o">(</span>Bytes<span class="o">))</span>
</code></pre></div>

<p>The compiler converts it into the same SASS instruction as can be seen in the CuBLAS code:</p>
<div class="highlight"><pre><span></span><code>LDGSTS.E.BYPASS.LTC128B.128<span class="w"> </span><span class="o">[</span>R11<span class="o">]</span>,<span class="w"> </span><span class="o">[</span>R2.64<span class="o">]</span>
</code></pre></div>

<p>Because the load is non-blocking, a separate memory fence is needed to synchronize the threads. As stated in the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=async#data-movement-and-conversion-instructions-asynchronous-copy">PTX manual</a>, asynchronous copies need to be committed to a group and waited for. The following two macros, comprising PTX instructions, do exactly that:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span>CP_ASYNC_COMMIT_GROUP<span class="o">()</span><span class="p">;</span>
<span class="w">    </span>CP_ASYNC_WAIT_GROUP<span class="o">(</span><span class="m">0</span><span class="o">)</span><span class="p">;</span>
</code></pre></div>

<p>These two macros get compiled into the following SASS code:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span>LDGDEPBAR
<span class="w">    </span>DEPBAR.LE<span class="w"> </span>SB0,<span class="w"> </span>0x0
</code></pre></div>

<p>These two SASS instructions are found in the CuBLAS code too. The slight difference between the two is covered in the next section. Visualizing the new load instruction <code>LDGSTS.E.BYPASS.LTC128B.128</code> is very instructive:</p>
<p><img alt="Async" src="https://fabianschuetze.github.io/images/async.png"></p>
<p>The data goes directly from the L2 Cache through the shared memory (a portion of the L1 cache).</p>
<h3>Overlapping Memory Loads with Computation</h3>
<p>The gift of asynchronous copy operations is that one can overlay computation with memory transfers and avoid pipeline stalls. The kernel can be expressed as:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">counter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">CP_ASYNC_COMMIT_GROUP</span><span class="p">();</span>
<span class="w">    </span><span class="n">CP_ASYNC_WAIT_GROUP</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span>
<span class="w">         </span><span class="n">block</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">         </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">         </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">         </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">         </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">wmma_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span>
<span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">wmma_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">wmma_step</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">wmma_steps</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">wmma_step</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                 </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">                 </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">                 </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">                 </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">Bs</span><span class="p">.</span><span class="n">cols_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kN</span><span class="p">);</span>
<span class="w">                 </span><span class="n">matmul</span><span class="p">.</span><span class="n">compute</span><span class="p">();</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">         </span><span class="n">counter</span><span class="w"> </span><span class="o">^=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">         </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kM</span><span class="w"> </span><span class="o">*</span>
<span class="w">                             </span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">skew</span><span class="p">));</span>
<span class="w">         </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span>
<span class="w">                             </span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kN</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">skew</span><span class="p">));</span>
<span class="w">         </span><span class="n">CP_ASYNC_COMMIT_GROUP</span><span class="p">();</span>
<span class="w">         </span><span class="n">CP_ASYNC_WAIT_GROUP</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">         </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">bk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">bk</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span><span class="w"> </span><span class="n">bk</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">         </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">();</span>
<span class="w">         </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">         </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">Bs</span><span class="p">.</span><span class="n">cols_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kN</span><span class="p">);</span>
<span class="w">         </span><span class="n">matmul</span><span class="p">.</span><span class="n">compute</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>

<p>The computation starts by loading data from global to shared memory. The class loading data from shared to global memory manages two buffers. Data gets read from one buffer and stored in the other buffer. The main loop begins by initiating a global memory load. The matrix elements are then computed. Afterward, the threads block until the previously fetched memory has been loaded. In the loop's epilogue, the last outstanding matrix computation is conducted.</p>
<p>This kernel attains 73 TFLOPS, a 20 percent increase to the first kernel.</p>
<h2>4. Improvement II: DOUBLE BUFFERING</h2>
<p>The code above already improves the throughput of the kernel. However, it is still below the CuBLAS version, and the assembly instructions do not match. In particular, the memory barrier in the code above is <code>DEPBAR.LE SB0, 0x0</code>, but the memory barrier in the CuBLAS code is <code>DEPBAR.LE SB0, 0x1</code>. The SASS instructions are undocumented, but one can assume that LE stands for less or equal. Furthermore, the PTX docs for the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=async#data-movement-and-conversion-instructions-cp-async-wait-group-cp-async-wait-all">memory barrier</a> state that the PTX instruction <code>cp.async.wait_group N</code> is:</p>
<blockquote>
<p>cp.async.wait_group instruction will cause the executing thread to wait till only N or fewer of the most recent cp.async-groups are pending and all the prior cp.async-groups committed by the executing threads are complete.</p>
</blockquote>
<p>Besides the difference in instructions, the kernel above also regularly stalled because data was unavailable. The warps stalled for almost two cycles for each issued instruction because data was unavailable (long scoreboard stall). To avoid such stalls and replicate the SASS code for the CuBLAS kernel, the kernel below does “double buffering”: Always have two shared memory operations in flight and await only the oldest one. Register loads are buffered too. The kernel has one register file loaded, loads the next one, and computes the matrix operation on the previous register file. The code for the kernel is as follows:</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">CP_ASYNC_COMMIT_GROUP</span><span class="p">();</span>
<span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">    </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="n">CP_ASYNC_COMMIT_GROUP</span><span class="p">();</span>
<span class="w">    </span><span class="n">CP_ASYNC_WAIT_GROUP</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="c1">// 1 = Wait until 1 recent async groups are pending</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">    </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">SpanTypeB</span><span class="o">::</span><span class="n">cols_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kN</span><span class="p">);</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">counter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span>
<span class="w">        </span><span class="n">block</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">wmma_steps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">;</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">wmma_steps</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">                    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">next</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">                    </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">next</span><span class="p">);</span>
<span class="w">                    </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">next</span><span class="p">);</span>
<span class="w">                    </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">                    </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">SpanTypeB</span><span class="o">::</span><span class="n">cols_</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMAblock</span><span class="o">::</span><span class="n">kN</span><span class="p">);</span>
<span class="w">                    </span><span class="n">matmul</span><span class="p">.</span><span class="n">compute</span><span class="p">(</span><span class="n">current</span><span class="p">);</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">                        </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">^</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">                        </span><span class="n">LoaderA</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="p">);</span>
<span class="w">                        </span><span class="n">LoaderB</span><span class="p">.</span><span class="n">next</span><span class="p">(</span><span class="n">Threadblock</span><span class="o">::</span><span class="n">kK</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">                        </span><span class="n">CP_ASYNC_COMMIT_GROUP</span><span class="p">();</span>
<span class="w">                        </span><span class="n">CP_ASYNC_WAIT_GROUP</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">                        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">                        </span><span class="n">RegisterLoaderA</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">MemLoaderA</span><span class="o">::</span><span class="n">size_</span><span class="p">);</span>
<span class="w">                        </span><span class="n">RegisterLoaderB</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">counter</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">MemLoaderB</span><span class="o">::</span><span class="n">size_</span><span class="p">);</span>
<span class="w">                        </span><span class="n">counter</span><span class="w"> </span><span class="o">^=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>

<p>The prologue to the main loop begins by issuing two shared memory loads. The threads block until only the first one is completed, while the second one remains in flight. Then, the first register file is loaded. The main loop begins by loading a further fragment of shared memory, and the tensor cores operate on the previous fragment. When all local registers are filled, the shared memory of the first block has been exhausted. No computation can be overlaid over the memory copies anymore. Another load is issued, and the warps wait until the previous load is completed.</p>
<p>With these advances, the throughput of the kernel advances to 89 TFLOPS and reaches within 95% of CuBLAS performance. Further gains can be reaped by writing the matrix C through shared memory back to global memory. The kernel throughput then advances to 91 TFLOPS, 1 TFLOP behind the CuBlas kernel.</p>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="https://fabianschuetze.github.io/bankconflictscuda.html" rel="bookmark"
                           title="Permalink to "BankConflictsCuda"">"BankConflictsCuda"</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2024-02-16T00:00:00+01:00">
                Published: Fr 16 Februar 2024
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://fabianschuetze.github.io/author/fabian-schuetze.html">Fabian Schuetze</a>
        </address>
<p>In <a href="https://fabianschuetze.github.io/category/articles.html">articles</a>.</p>

</footer><!-- /.post-info -->                <h1>Avoiding Memory Bank Conflicts in Cuda Programs</h1>
<p>Global memory access latencies are very high (relative to computing latencies) and a common source of under-utilization of CPUs and GPUs. Accessing memory efficiently is particularly important for harnessing the power of vector processors, such as SIMD processors and GPUs. Memory banks have …</p>
                <a class="readmore" href="https://fabianschuetze.github.io/bankconflictscuda.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>